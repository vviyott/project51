"""
ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘ ì•± - ì „ì²´ ê¸°ëŠ¥ (LangGraph ì—†ìŒ)
"""

import streamlit as st
import pandas as pd
from supabase import create_client
from openai import OpenAI
import os
from dotenv import load_dotenv
from datetime import datetime
import time
import json
import re
import requests
from bs4 import BeautifulSoup
import numpy as np

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘",
    page_icon="ğŸ›’",
    layout="wide"
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL") or st.secrets.get("SUPABASE_URL", "")
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or st.secrets.get("SUPABASE_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID") or st.secrets.get("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET") or st.secrets.get("NAVER_CLIENT_SECRET", "")

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .pros-section {
        background-color: #d4edda;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 5px solid #28a745;
    }
    .cons-section {
        background-color: #f8d7da;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 5px solid #dc3545;
    }
    .search-info {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
        border-left: 3px solid #2196f3;
    }
</style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ›’ ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘</h1>
    <p style="font-size: 1.2rem; margin-top: 1rem;">
        ë¸”ë¡œê·¸ì—ì„œ ìˆ˜ì§‘í•œ ì‹¤ì‚¬ìš© í›„ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œí’ˆì˜ ì¥ì ê³¼ ë‹¨ì ì„ í•œëˆˆì— í™•ì¸í•˜ì„¸ìš”
    </p>
</div>
""", unsafe_allow_html=True)

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_openai_client():
    return OpenAI(api_key=OPENAI_API_KEY)

@st.cache_resource
def get_supabase_client():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

# OpenAI ì„ë² ë”© í´ë˜ìŠ¤
class OpenAIEmbeddings:
    def __init__(self):
        self.client = get_openai_client()
        self.model = "text-embedding-ada-002"
    
    def get_embedding(self, text: str):
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            return response.data[0].embedding
        except Exception as e:
            st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰
def search_naver_blog(query, display=10):
    url = "https://openapi.naver.com/v1/search/blog"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": query, "display": display, "sort": "sim"}
    
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            return response.json()
    except Exception as e:
        st.error(f"ë¸”ë¡œê·¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    return None

# HTML íƒœê·¸ ì œê±°
def remove_html_tags(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r'<[^>]+>', '', text)
    return text.strip()

# ë¸”ë¡œê·¸ ë‚´ìš© í¬ë¡¤ë§
def crawl_blog_content(url):
    try:
        if "blog.naver.com" in url:
            parts = url.split('/')
            if len(parts) >= 5:
                blog_id = parts[3]
                post_no = parts[4].split('?')[0]
                mobile_url = f"https://m.blog.naver.com/{blog_id}/{post_no}"
                
                response = requests.get(mobile_url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    for selector in ['div.se-main-container', 'div#postViewArea', 'div.post_ct']:
                        elem = soup.select_one(selector)
                        if elem:
                            content = elem.get_text(separator='\n', strip=True)
                            content = re.sub(r'\s+', ' ', content)
                            return content if len(content) > 300 else None
    except Exception as e:
        st.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    return None

# GPTë¡œ ì¥ë‹¨ì  ì¶”ì¶œ
def extract_pros_cons(product_name, content):
    if not content or len(content) < 200:
        return None
    
    client = get_openai_client()
    prompt = f"""ë‹¤ìŒì€ "{product_name}"ì— ëŒ€í•œ ë¸”ë¡œê·¸ ë¦¬ë·°ì…ë‹ˆë‹¤.

[ë¸”ë¡œê·¸ ë‚´ìš©]
{content[:1500]}

ìœ„ ë‚´ìš©ì—ì„œ {product_name}ì˜ ì¥ì ê³¼ ë‹¨ì ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì¥ì :
- (êµ¬ì²´ì ì¸ ì¥ì )

ë‹¨ì :
- (êµ¬ì²´ì ì¸ ë‹¨ì )

ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì •ë³´ ë¶€ì¡±"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”."""
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë…¸íŠ¸ë¶ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=500
        )
        
        result = response.choices[0].message.content.strip()
        
        if result and "ì •ë³´ ë¶€ì¡±" not in result:
            pros = []
            cons = []
            
            lines = result.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                if 'ì¥ì :' in line:
                    current_section = 'pros'
                elif 'ë‹¨ì :' in line:
                    current_section = 'cons'
                elif line.startswith('-') and current_section:
                    point = line[1:].strip()
                    if point and len(point) > 5:
                        if current_section == 'pros':
                            pros.append(point)
                        else:
                            cons.append(point)
            
            if pros or cons:
                return {'pros': pros[:5], 'cons': cons[:5]}
    except Exception as e:
        st.error(f"GPT ë¶„ì„ ì˜¤ë¥˜: {e}")
    return None

# ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ (ìœ ì‚¬ë„ ê²€ìƒ‰ í¬í•¨)
def search_database(product_name):
    supabase = get_supabase_client()
    embeddings_helper = OpenAIEmbeddings()
    
    try:
        # 1. ì •í™•í•œ ë§¤ì¹­
        result = supabase.table('laptop_pros_cons').select("*").eq('product_name', product_name).execute()
        if result.data:
            return process_db_results(result.data), "exact", None
        
        # 2. ë¶€ë¶„ ë§¤ì¹­
        result = supabase.table('laptop_pros_cons').select("*").ilike('product_name', f'%{product_name}%').execute()
        if result.data:
            return process_db_results(result.data), "partial", None
        
        # 3. ìœ ì‚¬ë„ ê²€ìƒ‰ (ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš°)
        query_embedding = embeddings_helper.get_embedding(product_name)
        if query_embedding:
            all_products = supabase.table('laptop_pros_cons').select("*").execute()
            
            similar_products = []
            checked_products = set()
            
            for item in all_products.data:
                if item['product_name'] in checked_products:
                    continue
                
                if item.get('embedding'):
                    try:
                        item_embedding = json.loads(item['embedding']) if isinstance(item['embedding'], str) else item['embedding']
                        similarity = embeddings_helper.cosine_similarity(query_embedding, item_embedding)
                        
                        if similarity >= 0.7:
                            similar_products.append({
                                'product_name': item['product_name'],
                                'similarity': similarity
                            })
                            checked_products.add(item['product_name'])
                    except:
                        pass
            
            if similar_products:
                similar_products.sort(key=lambda x: x['similarity'], reverse=True)
                best_match = similar_products[0]['product_name']
                result = supabase.table('laptop_pros_cons').select("*").eq('product_name', best_match).execute()
                if result.data:
                    return process_db_results(result.data), "similarity", best_match
        
    except Exception as e:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
    
    return None, None, None

def process_db_results(data):
    pros = [item['content'] for item in data if item['type'] == 'pro']
    cons = [item['content'] for item in data if item['type'] == 'con']
    return {'pros': pros, 'cons': cons}

# ì›¹ í¬ë¡¤ë§ ë° ë¶„ì„
def crawl_and_analyze(product_name):
    all_pros = []
    all_cons = []
    sources = []
    
    search_queries = [
        f"{product_name} ì¥ë‹¨ì  ì‹¤ì‚¬ìš©",
        f"{product_name} í›„ê¸°"
    ]
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_posts = 6  # ê° ì¿¼ë¦¬ë‹¹ 3ê°œì”©
    current_post = 0
    
    for query in search_queries[:2]:
        status_text.text(f"'{query}' ê²€ìƒ‰ ì¤‘...")
        blog_result = search_naver_blog(query, display=5)
        
        if not blog_result or 'items' not in blog_result:
            continue
        
        for post in blog_result['items'][:3]:
            current_post += 1
            progress_bar.progress(current_post / total_posts)
            
            post['title'] = remove_html_tags(post['title'])
            status_text.text(f"ë¶„ì„ ì¤‘: {post['title'][:30]}...")
            
            content = crawl_blog_content(post['link'])
            
            if content:
                pros_cons = extract_pros_cons(product_name, content)
                if pros_cons:
                    all_pros.extend(pros_cons['pros'])
                    all_cons.extend(pros_cons['cons'])
                    sources.append({
                        'title': post['title'],
                        'link': post['link']
                    })
            
            time.sleep(0.5)
    
    progress_bar.empty()
    status_text.empty()
    
    # ì¤‘ë³µ ì œê±°
    pros = list(dict.fromkeys(all_pros))[:10]
    cons = list(dict.fromkeys(all_cons))[:10]
    
    return {'pros': pros, 'cons': cons}, sources[:5]

# ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
def save_to_database(product_name, pros, cons):
    supabase = get_supabase_client()
    embeddings_helper = OpenAIEmbeddings()
    
    try:
        # ì„ë² ë”© ìƒì„±
        embedding = embeddings_helper.get_embedding(product_name)
        
        data = []
        for pro in pros:
            data.append({
                'product_name': product_name,
                'type': 'pro',
                'content': pro,
                'embedding': embedding
            })
        
        for con in cons:
            data.append({
                'product_name': product_name,
                'type': 'con',
                'content': con,
                'embedding': embedding
            })
        
        if data:
            supabase.table('laptop_pros_cons').insert(data).execute()
            st.success("âœ… ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        st.error(f"ì €ì¥ ì˜¤ë¥˜: {e}")

# ë©”ì¸ ê²€ìƒ‰ UI
col1, col2, col3 = st.columns([1, 3, 1])

with col2:
    product_name = st.text_input(
        "ğŸ” ì œí’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ: ë§¥ë¶ í”„ë¡œ M3, LG ê·¸ë¨ 2024, ê°¤ëŸ­ì‹œë¶4 í”„ë¡œ"
    )
    
    search_button = st.button("ê²€ìƒ‰í•˜ê¸°", use_container_width=True)

# ê²€ìƒ‰ ì‹¤í–‰
if search_button and product_name:
    with st.spinner(f"'{product_name}' ê²€ìƒ‰ ì¤‘..."):
        # 1. ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
        db_results, match_type, similar_product = search_database(product_name)
        
        if db_results:
            # DBì—ì„œ ì°¾ì€ ê²½ìš°
            if match_type == "exact":
                st.success(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}' ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            elif match_type == "partial":
                st.info(f"ğŸ“Œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ì œí’ˆ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            else:  # similarity
                st.info(f"ğŸ¤– AI ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ '{similar_product}' ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            
            results = db_results
            sources = []
            search_method = "database"
        else:
            # ì›¹ í¬ë¡¤ë§
            st.warning(f"ğŸ”„ ì›¹ì—ì„œ '{product_name}' ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘...")
            results, sources = crawl_and_analyze(product_name)
            search_method = "web_crawling"
            
            # ê²°ê³¼ê°€ ìˆìœ¼ë©´ DBì— ì €ì¥
            if results['pros'] or results['cons']:
                save_to_database(product_name, results['pros'], results['cons'])
    
    # ê²°ê³¼ í‘œì‹œ
    if results and (results['pros'] or results['cons']):
        # ê²€ìƒ‰ ì •ë³´ í‘œì‹œ
        st.markdown(f"""
        <div class="search-info">
            <strong>ê²€ìƒ‰ ë°©ë²•:</strong> {'ë°ì´í„°ë² ì´ìŠ¤' if search_method == 'database' else 'ì›¹ í¬ë¡¤ë§'} | 
            <strong>ì¥ì :</strong> {len(results['pros'])}ê°œ | 
            <strong>ë‹¨ì :</strong> {len(results['cons'])}ê°œ
        </div>
        """, unsafe_allow_html=True)
        
        # ì¥ë‹¨ì  í‘œì‹œ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div class="pros-section">
                <h3>âœ… ì¥ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            for idx, pro in enumerate(results['pros'], 1):
                st.write(f"{idx}. {pro}")
        
        with col2:
            st.markdown("""
            <div class="cons-section">
                <h3>âŒ ë‹¨ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            for idx, con in enumerate(results['cons'], 1):
                st.write(f"{idx}. {con}")
        
        # ì¶œì²˜ (ì›¹ í¬ë¡¤ë§ì¸ ê²½ìš°)
        if sources:
            with st.expander("ğŸ“š ì¶œì²˜ ë³´ê¸°"):
                for idx, source in enumerate(sources, 1):
                    st.write(f"{idx}. [{source['title']}]({source['link']})")
        
        # í†µê³„
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ì¥ì ", f"{len(results['pros'])}ê°œ")
        with col2:
            st.metric("ì´ ë‹¨ì ", f"{len(results['cons'])}ê°œ")
        with col3:
            st.metric("ê²€ìƒ‰ ë°©ë²•", "DB" if search_method == "database" else "ì›¹")
    else:
        st.error(f"'{product_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    st.info("ğŸ’¡ í•œ ë²ˆ ê²€ìƒ‰ëœ ì œí’ˆì€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì–´ ë” ë¹ ë¥´ê²Œ ê²€ìƒ‰ë©ë‹ˆë‹¤.")
with col2:
    st.info("ğŸ¤– OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ì œí’ˆë„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤.")

current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
st.markdown(f"""
<div style="text-align: center; color: #666; padding: 2rem;">
    <p>ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {current_date}</p>
    <p>Powered by OpenAI & Naver API</p>
</div>
""", unsafe_allow_html=True)
